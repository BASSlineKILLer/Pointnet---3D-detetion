2025-07-12 09:49:45,370   INFO  **********************Start logging**********************
2025-07-12 09:49:45,370   INFO  CUDA_VISIBLE_DEVICES=ALL
2025-07-12 09:49:45,370   INFO  Training with a single process
2025-07-12 09:49:45,370   INFO  cfg_file         ./cfgs/nuscenes_models/cbgs_voxel0075_voxelnext.yaml
2025-07-12 09:49:45,370   INFO  batch_size       4
2025-07-12 09:49:45,370   INFO  epochs           100
2025-07-12 09:49:45,370   INFO  workers          4
2025-07-12 09:49:45,370   INFO  extra_tag        default
2025-07-12 09:49:45,370   INFO  ckpt             None
2025-07-12 09:49:45,370   INFO  pretrained_model None
2025-07-12 09:49:45,371   INFO  launcher         none
2025-07-12 09:49:45,371   INFO  tcp_port         18888
2025-07-12 09:49:45,371   INFO  sync_bn          False
2025-07-12 09:49:45,371   INFO  fix_random_seed  False
2025-07-12 09:49:45,371   INFO  ckpt_save_interval 1
2025-07-12 09:49:45,371   INFO  local_rank       None
2025-07-12 09:49:45,371   INFO  max_ckpt_save_num 30
2025-07-12 09:49:45,371   INFO  merge_all_iters_to_one_epoch False
2025-07-12 09:49:45,371   INFO  set_cfgs         None
2025-07-12 09:49:45,371   INFO  max_waiting_mins 0
2025-07-12 09:49:45,371   INFO  start_epoch      0
2025-07-12 09:49:45,371   INFO  num_epochs_to_eval 0
2025-07-12 09:49:45,371   INFO  save_to_file     False
2025-07-12 09:49:45,371   INFO  use_tqdm_to_record False
2025-07-12 09:49:45,371   INFO  logger_iter_interval 50
2025-07-12 09:49:45,371   INFO  ckpt_save_time_interval 300
2025-07-12 09:49:45,371   INFO  wo_gpu_stat      False
2025-07-12 09:49:45,371   INFO  use_amp          False
2025-07-12 09:49:45,371   INFO  cfg.ROOT_DIR: /root/autodl-tmp/OpenPCDet
2025-07-12 09:49:45,371   INFO  cfg.LOCAL_RANK: 0
2025-07-12 09:49:45,371   INFO  cfg.CLASS_NAMES: ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
2025-07-12 09:49:45,371   INFO  ----------- DATA_CONFIG -----------
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.DATASET: NuScenesDataset
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.DATA_PATH: ../data/nuscenes
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.VERSION: v1.0-mini
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.MAX_SWEEPS: 10
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.PRED_VELOCITY: True
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.SET_NAN_VELOCITY_TO_ZEROS: True
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.FILTER_MIN_POINTS_IN_GT: 1
2025-07-12 09:49:45,371   INFO  ----------- DATA_SPLIT -----------
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val
2025-07-12 09:49:45,371   INFO  ----------- INFO_PATH -----------
2025-07-12 09:49:45,371   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['nuscenes_infos_10sweeps_train.pkl']
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['nuscenes_infos_10sweeps_val.pkl']
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.BALANCED_RESAMPLING: True
2025-07-12 09:49:45,372   INFO  ----------- DATA_AUGMENTOR -----------
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'DB_INFO_PATH': ['nuscenes_dbinfos_10sweeps_withvelo.pkl'], 'USE_SHARED_MEMORY': False, 'DB_DATA_PATH': ['nuscenes_dbinfos_10sweeps_withvelo_global.pkl.npy'], 'PREPARE': {'filter_by_min_points': ['car:5', 'truck:5', 'construction_vehicle:5', 'bus:5', 'trailer:5', 'barrier:5', 'motorcycle:5', 'bicycle:5', 'pedestrian:5', 'traffic_cone:5']}, 'SAMPLE_GROUPS': ['car:2', 'truck:2', 'construction_vehicle:2', 'bus:2', 'trailer:2', 'barrier:2', 'motorcycle:2', 'bicycle:2', 'pedestrian:2', 'traffic_cone:2'], 'NUM_POINT_FEATURES': 5, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': True}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x', 'y']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.9, 1.1]}, {'NAME': 'random_world_translation', 'NOISE_TRANSLATE_STD': [0.5, 0.5, 0.5]}]
2025-07-12 09:49:45,372   INFO  ----------- POINT_FEATURE_ENCODING -----------
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity', 'timestamp']
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': True}}, {'NAME': 'transform_points_to_voxels', 'VOXEL_SIZE': [0.075, 0.075, 0.2], 'MAX_POINTS_PER_VOXEL': 10, 'MAX_NUMBER_OF_VOXELS': {'train': 120000, 'test': 160000}}]
2025-07-12 09:49:45,372   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
2025-07-12 09:49:45,372   INFO  ----------- MODEL -----------
2025-07-12 09:49:45,372   INFO  cfg.MODEL.NAME: VoxelNeXt
2025-07-12 09:49:45,372   INFO  ----------- VFE -----------
2025-07-12 09:49:45,372   INFO  cfg.MODEL.VFE.NAME: MeanVFE
2025-07-12 09:49:45,372   INFO  ----------- BACKBONE_3D -----------
2025-07-12 09:49:45,372   INFO  cfg.MODEL.BACKBONE_3D.NAME: VoxelResBackBone8xVoxelNeXt
2025-07-12 09:49:45,372   INFO  ----------- DENSE_HEAD -----------
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.NAME: VoxelNeXtHead
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.INPUT_FEATURES: 128
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.CLASS_NAMES_EACH_HEAD: [['car'], ['truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier'], ['motorcycle', 'bicycle'], ['pedestrian', 'traffic_cone']]
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.SHARED_CONV_CHANNEL: 128
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.KERNEL_SIZE_HEAD: 1
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.USE_BIAS_BEFORE_NORM: True
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.NUM_HM_CONV: 2
2025-07-12 09:49:45,372   INFO  ----------- SEPARATE_HEAD_CFG -----------
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_ORDER: ['center', 'center_z', 'dim', 'rot', 'vel']
2025-07-12 09:49:45,372   INFO  ----------- HEAD_DICT -----------
2025-07-12 09:49:45,372   INFO  ----------- center -----------
2025-07-12 09:49:45,372   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.out_channels: 2
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center.num_conv: 2
2025-07-12 09:49:45,373   INFO  ----------- center_z -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center_z.out_channels: 1
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.center_z.num_conv: 2
2025-07-12 09:49:45,373   INFO  ----------- dim -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.out_channels: 3
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.dim.num_conv: 2
2025-07-12 09:49:45,373   INFO  ----------- rot -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.out_channels: 2
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.rot.num_conv: 2
2025-07-12 09:49:45,373   INFO  ----------- vel -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.out_channels: 2
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.SEPARATE_HEAD_CFG.HEAD_DICT.vel.num_conv: 2
2025-07-12 09:49:45,373   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.FEATURE_MAP_STRIDE: 8
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.NUM_MAX_OBJS: 500
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.GAUSSIAN_OVERLAP: 0.1
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MIN_RADIUS: 2
2025-07-12 09:49:45,373   INFO  ----------- LOSS_CONFIG -----------
2025-07-12 09:49:45,373   INFO  ----------- LOSS_WEIGHTS -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.loc_weight: 0.25
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2, 1.0, 1.0]
2025-07-12 09:49:45,373   INFO  ----------- POST_PROCESSING -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.SCORE_THRESH: 0.1
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.POST_CENTER_LIMIT_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.MAX_OBJ_PER_SAMPLE: 500
2025-07-12 09:49:45,373   INFO  ----------- NMS_CONFIG -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.2
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 1000
2025-07-12 09:49:45,373   INFO  cfg.MODEL.DENSE_HEAD.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 83
2025-07-12 09:49:45,373   INFO  ----------- POST_PROCESSING -----------
2025-07-12 09:49:45,373   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
2025-07-12 09:49:45,374   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti
2025-07-12 09:49:45,374   INFO  ----------- OPTIMIZATION -----------
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 4
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 20
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.LR: 0.003
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.PCT_START: 0.4
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.LR_WARMUP: False
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1
2025-07-12 09:49:45,374   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10
2025-07-12 09:49:45,374   INFO  cfg.TAG: cbgs_voxel0075_voxelnext
2025-07-12 09:49:45,374   INFO  cfg.EXP_GROUP_PATH: cfgs/nuscenes_models
2025-07-12 09:49:45,380   INFO  ----------- Create dataloader & network & optimizer -----------
2025-07-12 09:49:45,425   INFO  Database filter by min points car: 4082 => 3303
2025-07-12 09:49:45,426   INFO  Database filter by min points truck: 451 => 412
2025-07-12 09:49:45,426   INFO  Database filter by min points construction_vehicle: 174 => 161
2025-07-12 09:49:45,426   INFO  Database filter by min points bus: 337 => 309
2025-07-12 09:49:45,426   INFO  Database filter by min points trailer: 59 => 57
2025-07-12 09:49:45,426   INFO  Database filter by min points barrier: 1851 => 1741
2025-07-12 09:49:45,426   INFO  Database filter by min points motorcycle: 179 => 149
2025-07-12 09:49:45,426   INFO  Database filter by min points bicycle: 147 => 136
2025-07-12 09:49:45,427   INFO  Database filter by min points pedestrian: 3068 => 2799
2025-07-12 09:49:45,427   INFO  Database filter by min points traffic_cone: 773 => 635
2025-07-12 09:49:45,428   INFO  Loading NuScenes dataset
2025-07-12 09:49:45,450   INFO  Total samples for NuScenes dataset: 323
2025-07-12 09:49:45,454   INFO  Total samples after balanced resampling: 1630
2025-07-12 09:49:47,561   INFO  ==> Loading parameters from checkpoint /root/autodl-tmp/OpenPCDet/output/cfgs/nuscenes_models/cbgs_voxel0075_voxelnext/default/ckpt/checkpoint_epoch_3.pth to GPU
2025-07-12 09:49:47,713   INFO  ==> Loading optimizer parameters from checkpoint /root/autodl-tmp/OpenPCDet/output/cfgs/nuscenes_models/cbgs_voxel0075_voxelnext/default/ckpt/checkpoint_epoch_3.pth to GPU
2025-07-12 09:49:47,745   INFO  ==> Done
2025-07-12 09:49:47,749   INFO  ----------- Model VoxelNeXt created, param count: 7981398 -----------
2025-07-12 09:49:47,749   INFO  VoxelNeXt(
  (vfe): MeanVFE()
  (backbone_3d): VoxelResBackBone8xVoxelNeXt(
    (conv_input): SparseSequential(
      (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (conv1): SparseSequential(
      (0): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (conv2): SparseSequential(
      (0): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (conv3): SparseSequential(
      (0): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (conv4): SparseSequential(
      (0): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (conv5): SparseSequential(
      (0): SparseSequential(
        (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (conv6): SparseSequential(
      (0): SparseSequential(
        (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
    )
    (shared_conv): SparseSequential(
      (0): SubMConv2d(128, 128, kernel_size=[3, 3], stride=[1, 1], padding=[1, 1], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (map_to_bev_module): None
  (pfe): None
  (backbone_2d): None
  (dense_head): VoxelNeXtHead(
    (heads_list): ModuleList(
      (0): SeparateHead(
        (center): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (center_z): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (dim): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (rot): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (vel): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (hm): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
      )
      (1): SeparateHead(
        (center): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (center_z): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (dim): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (rot): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (vel): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (hm): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
      )
      (2): SeparateHead(
        (center): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (center_z): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (dim): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (rot): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (vel): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (hm): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
      )
      (3): SeparateHead(
        (center): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (center_z): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (dim): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (rot): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (vel): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (hm): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
      )
      (4): SeparateHead(
        (center): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (center_z): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (dim): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (rot): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (vel): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (hm): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
      )
      (5): SeparateHead(
        (center): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (center_z): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 1, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (dim): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 3, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (rot): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (vel): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
        (hm): Sequential(
          (0): SparseSequential(
            (0): SubMConv2d(128, 128, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SubMConv2d(128, 2, kernel_size=[1, 1], stride=[1, 1], padding=[0, 0], dilation=[1, 1], output_padding=[0, 0], algo=ConvAlgo.MaskImplicitGemm)
        )
      )
    )
    (hm_loss_func): FocalLossSparse()
    (reg_loss_func): RegLossSparse()
  )
  (point_head): None
  (roi_head): None
)
2025-07-12 09:49:47,754   INFO  **********************Start training cfgs/nuscenes_models/cbgs_voxel0075_voxelnext(default)**********************
2025-07-12 09:50:04,486   INFO  Train:    4/100 (  4%) [   0/408 (  0%)]  Loss: 12.78 (12.8)  LR: 3.373e-04  Time cost: 00:15/1:47:09 [00:16/173:14:07]  Acc_iter 1225        Data time: 5.14(5.14)  Forward time: 10.62(10.62)  Batch time: 15.76(15.76)
2025-07-12 09:50:18,741   INFO  Train:    4/100 (  4%) [  25/408 (  6%)]  Loss: 14.49 (13.8)  LR: 3.388e-04  Time cost: 00:30/07:22 [00:30/12:40:55]  Acc_iter 1250        Data time: 0.00(0.20)  Forward time: 0.65(0.95)  Batch time: 0.65(1.15)
2025-07-12 09:50:45,850   INFO  Train:    4/100 (  4%) [  75/408 ( 18%)]  Loss: 12.06 (13.6)  LR: 3.420e-04  Time cost: 00:57/04:10 [00:58/8:14:49]  Acc_iter 1300        Data time: 0.00(0.07)  Forward time: 0.47(0.68)  Batch time: 0.47(0.75)
2025-07-12 09:51:11,876   INFO  Train:    4/100 (  4%) [ 125/408 ( 31%)]  Loss: 12.76 (13.5)  LR: 3.453e-04  Time cost: 01:23/03:06 [01:24/7:13:53]  Acc_iter 1350        Data time: 0.00(0.04)  Forward time: 0.43(0.62)  Batch time: 0.44(0.66)
2025-07-12 09:51:12,128   INFO  
2025-07-12 09:51:37,376   INFO  Train:    4/100 (  4%) [ 175/408 ( 43%)]  Loss: 16.68 (13.4)  LR: 3.487e-04  Time cost: 01:48/02:23 [01:49/6:45:23]  Acc_iter 1400        Data time: 0.00(0.03)  Forward time: 0.42(0.58)  Batch time: 0.42(0.62)
2025-07-12 09:52:03,019   INFO  Train:    4/100 (  4%) [ 225/408 ( 55%)]  Loss: 12.85 (13.2)  LR: 3.522e-04  Time cost: 02:14/01:48 [02:15/6:29:42]  Acc_iter 1450        Data time: 0.00(0.03)  Forward time: 0.37(0.57)  Batch time: 0.37(0.59)
2025-07-12 09:52:28,183   INFO  Train:    4/100 (  4%) [ 275/408 ( 67%)]  Loss: 12.00 (13.1)  LR: 3.558e-04  Time cost: 02:39/01:16 [02:40/6:18:25]  Acc_iter 1500        Data time: 0.00(0.02)  Forward time: 0.65(0.55)  Batch time: 0.65(0.58)
2025-07-12 09:52:28,432   INFO  
2025-07-12 09:52:53,561   INFO  Train:    4/100 (  4%) [ 325/408 ( 80%)]  Loss: 12.84 (13.1)  LR: 3.596e-04  Time cost: 03:04/00:47 [03:05/6:10:54]  Acc_iter 1550        Data time: 0.00(0.02)  Forward time: 0.60(0.55)  Batch time: 0.61(0.57)
2025-07-12 09:53:18,648   INFO  Train:    4/100 (  4%) [ 375/408 ( 92%)]  Loss: 11.39 (12.9)  LR: 3.634e-04  Time cost: 03:29/00:18 [03:30/6:04:45]  Acc_iter 1600        Data time: 0.00(0.02)  Forward time: 0.26(0.54)  Batch time: 0.26(0.56)
2025-07-12 09:53:32,567   INFO  Train:    4/100 (  4%) [ 407/408 (100%)]  Loss: 10.94 (12.9)  LR: 3.660e-04  Time cost: 03:43/00:00 [03:44/5:58:09]  Acc_iter 1632        Data time: 0.00(0.02)  Forward time: 0.14(0.53)  Batch time: 0.14(0.55)
2025-07-12 09:53:37,213   INFO  Train:    5/100 (  5%) [   0/408 (  0%)]  Loss: 12.92 (12.9)  LR: 3.661e-04  Time cost: 00:03/26:08 [03:49/41:49:47]  Acc_iter 1633        Data time: 3.09(3.09)  Forward time: 0.75(0.75)  Batch time: 3.84(3.84)
2025-07-12 09:53:45,896   INFO  Train:    5/100 (  5%) [  17/408 (  4%)]  Loss: 12.98 (11.9)  LR: 3.674e-04  Time cost: 00:12/04:32 [03:58/7:34:09]  Acc_iter 1650        Data time: 0.00(0.18)  Forward time: 0.55(0.52)  Batch time: 0.55(0.70)
2025-07-12 09:53:46,184   INFO  
2025-07-12 09:54:11,374   INFO  Train:    5/100 (  5%) [  67/408 ( 16%)]  Loss: 11.23 (11.9)  LR: 3.716e-04  Time cost: 00:38/03:10 [04:23/6:04:14]  Acc_iter 1700        Data time: 0.00(0.05)  Forward time: 0.41(0.51)  Batch time: 0.41(0.56)
2025-07-12 09:54:37,741   INFO  Train:    5/100 (  5%) [ 117/408 ( 29%)]  Loss: 11.95 (11.9)  LR: 3.758e-04  Time cost: 01:04/02:38 [04:49/5:55:03]  Acc_iter 1750        Data time: 0.00(0.03)  Forward time: 0.34(0.51)  Batch time: 0.34(0.55)
2025-07-12 09:55:03,876   INFO  Train:    5/100 (  5%) [ 167/408 ( 41%)]  Loss: 10.82 (11.9)  LR: 3.801e-04  Time cost: 01:30/02:09 [05:16/5:50:11]  Acc_iter 1800        Data time: 0.01(0.02)  Forward time: 0.47(0.51)  Batch time: 0.48(0.54)
2025-07-12 09:55:04,131   INFO  
2025-07-12 09:55:28,920   INFO  Train:    5/100 (  5%) [ 217/408 ( 53%)]  Loss: 9.091 (11.9)  LR: 3.846e-04  Time cost: 01:55/01:41 [05:41/5:44:06]  Acc_iter 1850        Data time: 0.01(0.02)  Forward time: 0.49(0.51)  Batch time: 0.50(0.53)
2025-07-12 09:55:54,374   INFO  Train:    5/100 (  5%) [ 267/408 ( 65%)]  Loss: 10.96 (11.8)  LR: 3.892e-04  Time cost: 02:21/01:14 [06:06/5:41:07]  Acc_iter 1900        Data time: 0.00(0.02)  Forward time: 0.28(0.51)  Batch time: 0.28(0.53)
2025-07-12 09:56:20,283   INFO  Train:    5/100 (  5%) [ 317/408 ( 78%)]  Loss: 12.34 (11.8)  LR: 3.939e-04  Time cost: 02:46/00:47 [06:32/5:39:52]  Acc_iter 1950        Data time: 0.00(0.02)  Forward time: 0.27(0.51)  Batch time: 0.28(0.52)
2025-07-12 09:56:20,563   INFO  
2025-07-12 09:56:46,729   INFO  Train:    5/100 (  5%) [ 367/408 ( 90%)]  Loss: 10.64 (11.7)  LR: 3.987e-04  Time cost: 03:13/00:21 [06:58/5:39:47]  Acc_iter 2000        Data time: 0.00(0.01)  Forward time: 0.78(0.51)  Batch time: 0.78(0.53)
2025-07-12 09:57:05,080   INFO  Train:    5/100 (  5%) [ 407/408 (100%)]  Loss: 13.11 (11.7)  LR: 4.027e-04  Time cost: 03:31/00:00 [07:17/5:35:13]  Acc_iter 2040        Data time: 0.00(0.01)  Forward time: 0.14(0.51)  Batch time: 0.14(0.52)
2025-07-12 09:57:09,951   INFO  Train:    6/100 (  6%) [   0/408 (  0%)]  Loss: 11.64 (11.6)  LR: 4.028e-04  Time cost: 00:03/26:51 [07:22/42:31:51]  Acc_iter 2041        Data time: 3.08(3.08)  Forward time: 0.87(0.87)  Batch time: 3.95(3.95)
2025-07-12 09:57:14,560   INFO  Train:    6/100 (  6%) [   9/408 (  2%)]  Loss: 10.61 (10.8)  LR: 4.037e-04  Time cost: 00:08/05:41 [07:26/9:12:47]  Acc_iter 2050        Data time: 0.00(0.31)  Forward time: 0.58(0.54)  Batch time: 0.58(0.86)
2025-07-12 09:57:39,796   INFO  Train:    6/100 (  6%) [  59/408 ( 14%)]  Loss: 12.92 (10.8)  LR: 4.087e-04  Time cost: 00:33/03:16 [07:52/6:03:18]  Acc_iter 2100        Data time: 0.00(0.05)  Forward time: 0.32(0.51)  Batch time: 0.33(0.56)
2025-07-12 09:57:40,040   INFO  
2025-07-12 09:58:05,788   INFO  Train:    6/100 (  6%) [ 109/408 ( 27%)]  Loss: 12.26 (11.0)  LR: 4.139e-04  Time cost: 00:59/02:42 [08:18/5:50:07]  Acc_iter 2150        Data time: 0.00(0.03)  Forward time: 0.39(0.51)  Batch time: 0.40(0.54)
2025-07-12 09:58:31,792   INFO  Train:    6/100 (  6%) [ 159/408 ( 39%)]  Loss: 11.93 (11.0)  LR: 4.192e-04  Time cost: 01:25/02:13 [08:44/5:44:57]  Acc_iter 2200        Data time: 0.00(0.03)  Forward time: 0.59(0.51)  Batch time: 0.59(0.54)
2025-07-12 09:59:03,392   INFO  Train:    6/100 (  6%) [ 209/408 ( 51%)]  Loss: 12.18 (11.0)  LR: 4.246e-04  Time cost: 01:57/01:51 [09:15/5:59:10]  Acc_iter 2250        Data time: 0.00(0.02)  Forward time: 0.33(0.54)  Batch time: 0.33(0.56)
2025-07-12 09:59:03,645   INFO  
2025-07-12 09:59:29,748   INFO  Train:    6/100 (  6%) [ 259/408 ( 63%)]  Loss: 11.32 (11.0)  LR: 4.301e-04  Time cost: 02:23/01:22 [09:41/5:54:46]  Acc_iter 2300        Data time: 0.00(0.02)  Forward time: 0.47(0.53)  Batch time: 0.47(0.55)
2025-07-12 09:59:56,255   INFO  Train:    6/100 (  6%) [ 309/408 ( 76%)]  Loss: 10.96 (10.9)  LR: 4.357e-04  Time cost: 02:50/00:54 [10:08/5:51:57]  Acc_iter 2350        Data time: 0.00(0.02)  Forward time: 0.40(0.53)  Batch time: 0.40(0.55)
2025-07-12 10:00:22,181   INFO  Train:    6/100 (  6%) [ 359/408 ( 88%)]  Loss: 9.817 (10.9)  LR: 4.414e-04  Time cost: 03:16/00:26 [10:34/5:48:46]  Acc_iter 2400        Data time: 0.00(0.01)  Forward time: 0.69(0.53)  Batch time: 0.69(0.54)
2025-07-12 10:00:22,399   INFO  
2025-07-12 10:00:45,251   INFO  Train:    6/100 (  6%) [ 407/408 (100%)]  Loss: 11.72 (10.9)  LR: 4.470e-04  Time cost: 03:39/00:00 [10:57/5:43:30]  Acc_iter 2448        Data time: 0.00(0.01)  Forward time: 0.14(0.52)  Batch time: 0.15(0.54)
2025-07-12 10:00:49,595   INFO  Train:    7/100 (  7%) [   0/408 (  0%)]  Loss: 10.45 (10.4)  LR: 4.471e-04  Time cost: 00:03/24:19 [11:01/38:05:52]  Acc_iter 2449        Data time: 2.85(2.85)  Forward time: 0.73(0.73)  Batch time: 3.58(3.58)
2025-07-12 10:00:50,408   INFO  Train:    7/100 (  7%) [   1/408 (  0%)]  Loss: 9.272 (9.86)  LR: 4.473e-04  Time cost: 00:04/14:53 [11:02/23:22:58]  Acc_iter 2450        Data time: 0.00(1.43)  Forward time: 0.81(0.77)  Batch time: 0.81(2.19)
2025-07-12 10:01:16,849   INFO  Train:    7/100 (  7%) [  51/408 ( 12%)]  Loss: 9.822 (10.3)  LR: 4.532e-04  Time cost: 00:30/03:31 [11:29/6:18:28]  Acc_iter 2500        Data time: 0.00(0.06)  Forward time: 0.96(0.53)  Batch time: 0.97(0.59)
2025-07-12 10:01:42,791   INFO  Train:    7/100 (  7%) [ 101/408 ( 25%)]  Loss: 10.94 (10.3)  LR: 4.593e-04  Time cost: 00:56/02:50 [11:55/5:54:49]  Acc_iter 2550        Data time: 0.00(0.03)  Forward time: 0.66(0.52)  Batch time: 0.67(0.56)
2025-07-12 10:01:43,084   INFO  
2025-07-12 10:02:08,592   INFO  Train:    7/100 (  7%) [ 151/408 ( 37%)]  Loss: 9.231 (10.4)  LR: 4.655e-04  Time cost: 01:22/02:19 [12:20/5:45:52]  Acc_iter 2600        Data time: 0.00(0.02)  Forward time: 0.28(0.52)  Batch time: 0.28(0.54)
2025-07-12 10:02:34,063   INFO  Train:    7/100 (  7%) [ 201/408 ( 49%)]  Loss: 9.593 (10.4)  LR: 4.717e-04  Time cost: 01:48/01:50 [12:46/5:40:05]  Acc_iter 2650        Data time: 0.00(0.02)  Forward time: 0.50(0.52)  Batch time: 0.50(0.53)
2025-07-12 10:03:01,062   INFO  Train:    7/100 (  7%) [ 251/408 ( 62%)]  Loss: 9.483 (10.4)  LR: 4.781e-04  Time cost: 02:15/01:24 [13:13/5:40:17]  Acc_iter 2700        Data time: 0.02(0.02)  Forward time: 0.52(0.52)  Batch time: 0.54(0.54)
2025-07-12 10:03:01,346   INFO  
2025-07-12 10:03:26,767   INFO  Train:    7/100 (  7%) [ 301/408 ( 74%)]  Loss: 8.132 (10.4)  LR: 4.847e-04  Time cost: 02:40/00:56 [13:39/5:37:33]  Acc_iter 2750        Data time: 0.00(0.02)  Forward time: 0.34(0.52)  Batch time: 0.34(0.53)
2025-07-12 10:03:51,679   INFO  Train:    7/100 (  7%) [ 351/408 ( 86%)]  Loss: 10.63 (10.4)  LR: 4.913e-04  Time cost: 03:05/00:30 [14:03/5:34:03]  Acc_iter 2800        Data time: 0.00(0.01)  Forward time: 0.35(0.51)  Batch time: 0.36(0.53)
2025-07-12 10:04:16,977   INFO  Train:    7/100 (  7%) [ 401/408 ( 98%)]  Loss: 7.336 (10.4)  LR: 4.980e-04  Time cost: 03:30/00:03 [14:29/5:31:55]  Acc_iter 2850        Data time: 0.03(0.01)  Forward time: 0.29(0.51)  Batch time: 0.31(0.52)
2025-07-12 10:04:17,172   INFO  
2025-07-12 10:04:18,672   INFO  Train:    7/100 (  7%) [ 407/408 (100%)]  Loss: 9.224 (10.4)  LR: 4.988e-04  Time cost: 03:32/00:00 [14:30/5:29:37]  Acc_iter 2856        Data time: 0.00(0.01)  Forward time: 0.16(0.51)  Batch time: 0.16(0.52)
2025-07-12 10:04:22,972   INFO  Train:    8/100 (  8%) [   0/408 (  0%)]  Loss: 9.206 (9.21)  LR: 4.989e-04  Time cost: 00:03/24:10 [14:35/37:28:36]  Acc_iter 2857        Data time: 2.42(2.42)  Forward time: 1.13(1.13)  Batch time: 3.56(3.56)
2025-07-12 10:04:45,159   INFO  Train:    8/100 (  8%) [  43/408 ( 11%)]  Loss: 10.44 (9.94)  LR: 5.048e-04  Time cost: 00:25/03:33 [14:57/6:09:33]  Acc_iter 2900        Data time: 0.00(0.06)  Forward time: 0.30(0.53)  Batch time: 0.30(0.59)
2025-07-12 10:05:11,847   INFO  Train:    8/100 (  8%) [  93/408 ( 23%)]  Loss: 9.902 (10.1)  LR: 5.117e-04  Time cost: 00:52/02:55 [15:24/5:51:52]  Acc_iter 2950        Data time: 0.01(0.03)  Forward time: 0.67(0.53)  Batch time: 0.68(0.56)
2025-07-12 10:05:38,043   INFO  Train:    8/100 (  8%) [ 143/408 ( 35%)]  Loss: 8.653 (10.1)  LR: 5.188e-04  Time cost: 01:18/02:24 [15:50/5:43:59]  Acc_iter 3000        Data time: 0.00(0.02)  Forward time: 0.78(0.52)  Batch time: 0.79(0.55)
2025-07-12 10:05:38,282   INFO  
2025-07-12 10:06:04,103   INFO  Train:    8/100 (  8%) [ 193/408 ( 47%)]  Loss: 10.06 (10.0)  LR: 5.259e-04  Time cost: 01:44/01:56 [16:16/5:39:31]  Acc_iter 3050        Data time: 0.00(0.02)  Forward time: 0.43(0.52)  Batch time: 0.43(0.54)
2025-07-12 10:06:29,909   INFO  Train:    8/100 (  8%) [ 243/408 ( 60%)]  Loss: 9.426 (10.0)  LR: 5.332e-04  Time cost: 02:10/01:28 [16:42/5:36:02]  Acc_iter 3100        Data time: 0.00(0.01)  Forward time: 0.29(0.52)  Batch time: 0.29(0.53)
2025-07-12 10:06:55,701   INFO  Train:    8/100 (  8%) [ 293/408 ( 72%)]  Loss: 8.781 (10.0)  LR: 5.405e-04  Time cost: 02:36/01:01 [17:07/5:33:34]  Acc_iter 3150        Data time: 0.02(0.01)  Forward time: 0.26(0.52)  Batch time: 0.28(0.53)
2025-07-12 10:06:55,987   INFO  
2025-07-12 10:07:21,765   INFO  Train:    8/100 (  8%) [ 343/408 ( 84%)]  Loss: 9.718 (9.95)  LR: 5.480e-04  Time cost: 03:02/00:34 [17:34/5:32:11]  Acc_iter 3200        Data time: 0.00(0.01)  Forward time: 0.72(0.52)  Batch time: 0.72(0.53)
2025-07-12 10:07:47,859   INFO  Train:    8/100 (  8%) [ 393/408 ( 96%)]  Loss: 9.272 (9.88)  LR: 5.555e-04  Time cost: 03:28/00:07 [18:00/5:31:06]  Acc_iter 3250        Data time: 0.00(0.01)  Forward time: 0.26(0.52)  Batch time: 0.27(0.53)
2025-07-12 10:07:53,253   INFO  Train:    8/100 (  8%) [ 407/408 (100%)]  Loss: 11.50 (9.87)  LR: 5.577e-04  Time cost: 03:33/00:00 [18:05/5:27:53]  Acc_iter 3264        Data time: 0.00(0.01)  Forward time: 0.14(0.51)  Batch time: 0.14(0.52)
2025-07-12 10:07:57,332   INFO  Train:    9/100 (  9%) [   0/408 (  0%)]  Loss: 11.36 (11.4)  LR: 5.578e-04  Time cost: 00:03/22:24 [18:09/34:22:14]  Acc_iter 3265        Data time: 2.50(2.50)  Forward time: 0.79(0.79)  Batch time: 3.30(3.30)
2025-07-12 10:08:15,702   INFO  Train:    9/100 (  9%) [  35/408 (  9%)]  Loss: 8.763 (9.86)  LR: 5.632e-04  Time cost: 00:21/03:44 [18:27/6:16:10]  Acc_iter 3300        Data time: 0.00(0.07)  Forward time: 0.71(0.53)  Batch time: 0.72(0.60)
2025-07-12 10:08:16,036   INFO  
2025-07-12 10:08:42,560   INFO  Train:    9/100 (  9%) [  85/408 ( 21%)]  Loss: 10.51 (9.40)  LR: 5.710e-04  Time cost: 00:48/03:02 [18:54/5:52:11]  Acc_iter 3350        Data time: 0.00(0.04)  Forward time: 0.37(0.53)  Batch time: 0.37(0.56)
2025-07-12 10:09:08,980   INFO  Train:    9/100 (  9%) [ 135/408 ( 33%)]  Loss: 11.30 (9.53)  LR: 5.788e-04  Time cost: 01:14/02:30 [19:21/5:43:30]  Acc_iter 3400        Data time: 0.00(0.03)  Forward time: 0.51(0.53)  Batch time: 0.51(0.55)
2025-07-12 10:09:34,458   INFO  Train:    9/100 (  9%) [ 185/408 ( 45%)]  Loss: 11.97 (9.57)  LR: 5.868e-04  Time cost: 01:40/02:00 [19:46/5:36:06]  Acc_iter 3450        Data time: 0.00(0.02)  Forward time: 0.76(0.52)  Batch time: 0.76(0.54)
2025-07-12 10:09:34,755   INFO  
2025-07-12 10:10:00,997   INFO  Train:    9/100 (  9%) [ 235/408 ( 58%)]  Loss: 8.397 (9.41)  LR: 5.948e-04  Time cost: 02:06/01:33 [20:13/5:34:26]  Acc_iter 3500        Data time: 0.00(0.02)  Forward time: 0.52(0.52)  Batch time: 0.52(0.54)
2025-07-12 10:10:26,688   INFO  Train:    9/100 (  9%) [ 285/408 ( 70%)]  Loss: 8.653 (9.38)  LR: 6.030e-04  Time cost: 02:32/01:05 [20:38/5:31:22]  Acc_iter 3550        Data time: 0.00(0.01)  Forward time: 0.61(0.52)  Batch time: 0.62(0.53)
2025-07-12 10:10:53,160   INFO  Train:    9/100 (  9%) [ 335/408 ( 82%)]  Loss: 8.831 (9.35)  LR: 6.112e-04  Time cost: 02:59/00:38 [21:05/5:30:32]  Acc_iter 3600        Data time: 0.00(0.01)  Forward time: 0.40(0.52)  Batch time: 0.40(0.53)
2025-07-12 10:10:53,425   INFO  
2025-07-12 10:11:18,919   INFO  Train:    9/100 (  9%) [ 385/408 ( 94%)]  Loss: 10.50 (9.38)  LR: 6.196e-04  Time cost: 03:24/00:12 [21:31/5:28:39]  Acc_iter 3650        Data time: 0.00(0.01)  Forward time: 0.46(0.52)  Batch time: 0.46(0.53)
2025-07-12 10:11:28,097   INFO  Train:    9/100 (  9%) [ 407/408 (100%)]  Loss: 8.389 (9.32)  LR: 6.233e-04  Time cost: 03:34/00:00 [21:40/5:24:40]  Acc_iter 3672        Data time: 0.01(0.01)  Forward time: 0.14(0.51)  Batch time: 0.15(0.52)
2025-07-12 10:11:33,066   INFO  Train:   10/100 ( 10%) [   0/408 (  0%)]  Loss: 9.406 (9.41)  LR: 6.235e-04  Time cost: 00:03/26:41 [21:45/40:29:25]  Acc_iter 3673        Data time: 3.06(3.06)  Forward time: 0.87(0.87)  Batch time: 3.93(3.93)
2025-07-12 10:11:46,627   INFO  Train:   10/100 ( 10%) [  27/408 (  7%)]  Loss: 9.036 (9.60)  LR: 6.280e-04  Time cost: 00:17/03:57 [21:58/6:26:10]  Acc_iter 3700        Data time: 0.00(0.11)  Forward time: 0.47(0.51)  Batch time: 0.47(0.62)
2025-07-12 10:12:12,065   INFO  Train:   10/100 ( 10%) [  77/408 ( 19%)]  Loss: 9.530 (9.04)  LR: 6.366e-04  Time cost: 00:42/03:02 [22:24/5:39:49]  Acc_iter 3750        Data time: 0.00(0.04)  Forward time: 0.39(0.51)  Batch time: 0.40(0.55)
2025-07-12 10:12:12,319   INFO  
2025-07-12 10:12:38,159   INFO  Train:   10/100 ( 10%) [ 127/408 ( 31%)]  Loss: 11.25 (9.05)  LR: 6.452e-04  Time cost: 01:09/02:31 [22:50/5:32:31]  Acc_iter 3800        Data time: 0.00(0.03)  Forward time: 0.60(0.51)  Batch time: 0.61(0.54)
2025-07-12 10:13:03,149   INFO  Train:   10/100 ( 10%) [ 177/408 ( 43%)]  Loss: 9.238 (8.94)  LR: 6.539e-04  Time cost: 01:34/02:01 [23:15/5:25:15]  Acc_iter 3850        Data time: 0.00(0.02)  Forward time: 0.50(0.51)  Batch time: 0.50(0.53)
2025-07-12 10:13:28,156   INFO  Train:   10/100 ( 10%) [ 227/408 ( 56%)]  Loss: 8.991 (9.05)  LR: 6.627e-04  Time cost: 01:59/01:34 [23:40/5:21:02]  Acc_iter 3900        Data time: 0.00(0.02)  Forward time: 0.35(0.50)  Batch time: 0.36(0.52)
2025-07-12 10:13:28,411   INFO  
2025-07-12 10:13:53,908   INFO  Train:   10/100 ( 10%) [ 277/408 ( 68%)]  Loss: 8.168 (9.06)  LR: 6.716e-04  Time cost: 02:24/01:08 [24:06/5:19:49]  Acc_iter 3950        Data time: 0.00(0.02)  Forward time: 0.28(0.50)  Batch time: 0.28(0.52)
2025-07-12 10:14:20,076   INFO  Train:   10/100 ( 10%) [ 327/408 ( 80%)]  Loss: 8.190 (9.02)  LR: 6.806e-04  Time cost: 02:50/00:42 [24:32/5:19:38]  Acc_iter 4000        Data time: 0.00(0.01)  Forward time: 0.91(0.51)  Batch time: 0.92(0.52)
2025-07-12 10:14:46,047   INFO  Train:   10/100 ( 10%) [ 377/408 ( 92%)]  Loss: 6.671 (8.94)  LR: 6.897e-04  Time cost: 03:16/00:16 [24:58/5:19:04]  Acc_iter 4050        Data time: 0.00(0.01)  Forward time: 0.63(0.51)  Batch time: 0.63(0.52)
2025-07-12 10:14:46,327   INFO  
2025-07-12 10:14:59,743   INFO  Train:   10/100 ( 10%) [ 407/408 (100%)]  Loss: 8.140 (8.92)  LR: 6.952e-04  Time cost: 03:30/00:00 [25:11/5:15:54]  Acc_iter 4080        Data time: 0.00(0.01)  Forward time: 0.24(0.50)  Batch time: 0.24(0.52)
2025-07-12 10:15:03,970   INFO  Train:   11/100 ( 11%) [   0/408 (  0%)]  Loss: 9.808 (9.81)  LR: 6.954e-04  Time cost: 00:03/22:43 [25:16/34:05:21]  Acc_iter 4081        Data time: 1.93(1.93)  Forward time: 1.41(1.41)  Batch time: 3.34(3.34)
2025-07-12 10:15:13,839   INFO  Train:   11/100 ( 11%) [  19/408 (  5%)]  Loss: 10.09 (9.12)  LR: 6.989e-04  Time cost: 00:13/04:16 [25:26/6:44:02]  Acc_iter 4100        Data time: 0.01(0.10)  Forward time: 0.51(0.56)  Batch time: 0.52(0.66)
2025-07-12 10:15:39,967   INFO  Train:   11/100 ( 11%) [  69/408 ( 17%)]  Loss: 7.254 (8.93)  LR: 7.082e-04  Time cost: 00:39/03:10 [25:52/5:43:17]  Acc_iter 4150        Data time: 0.01(0.03)  Forward time: 0.35(0.53)  Batch time: 0.35(0.56)
2025-07-12 10:16:06,018   INFO  Train:   11/100 ( 11%) [ 119/408 ( 29%)]  Loss: 8.078 (8.87)  LR: 7.175e-04  Time cost: 01:05/02:37 [26:18/5:32:24]  Acc_iter 4200        Data time: 0.00(0.02)  Forward time: 0.57(0.52)  Batch time: 0.58(0.54)
2025-07-12 10:16:06,291   INFO  
2025-07-12 10:16:32,044   INFO  Train:   11/100 ( 11%) [ 169/408 ( 41%)]  Loss: 7.846 (8.71)  LR: 7.270e-04  Time cost: 01:31/02:08 [26:44/5:27:34]  Acc_iter 4250        Data time: 0.00(0.02)  Forward time: 0.62(0.52)  Batch time: 0.62(0.54)
2025-07-12 10:16:57,392   INFO  Train:   11/100 ( 11%) [ 219/408 ( 54%)]  Loss: 9.526 (8.63)  LR: 7.365e-04  Time cost: 01:56/01:40 [27:09/5:22:52]  Acc_iter 4300        Data time: 0.01(0.01)  Forward time: 0.45(0.52)  Batch time: 0.46(0.53)
2025-07-12 10:17:23,202   INFO  Train:   11/100 ( 11%) [ 269/408 ( 66%)]  Loss: 7.472 (8.65)  LR: 7.461e-04  Time cost: 02:22/01:13 [27:35/5:20:47]  Acc_iter 4350        Data time: 0.02(0.01)  Forward time: 0.46(0.52)  Batch time: 0.48(0.53)
2025-07-12 10:17:23,468   INFO  
2025-07-12 10:17:49,177   INFO  Train:   11/100 ( 11%) [ 319/408 ( 78%)]  Loss: 7.476 (8.61)  LR: 7.558e-04  Time cost: 02:48/00:46 [28:01/5:19:33]  Acc_iter 4400        Data time: 0.01(0.01)  Forward time: 0.76(0.51)  Batch time: 0.76(0.53)
2025-07-12 10:18:13,768   INFO  Train:   11/100 ( 11%) [ 369/408 ( 90%)]  Loss: 7.929 (8.60)  LR: 7.656e-04  Time cost: 03:13/00:20 [28:26/5:16:15]  Acc_iter 4450        Data time: 0.00(0.01)  Forward time: 0.37(0.51)  Batch time: 0.38(0.52)
2025-07-12 10:18:31,867   INFO  Train:   11/100 ( 11%) [ 407/408 (100%)]  Loss: 10.25 (8.61)  LR: 7.730e-04  Time cost: 03:31/00:00 [28:44/5:13:20]  Acc_iter 4488        Data time: 0.00(0.01)  Forward time: 0.16(0.51)  Batch time: 0.17(0.52)
2025-07-12 10:18:37,037   INFO  Train:   12/100 ( 12%) [   0/408 (  0%)]  Loss: 6.501 (6.50)  LR: 7.732e-04  Time cost: 00:04/28:42 [28:49/42:34:22]  Acc_iter 4489        Data time: 3.29(3.29)  Forward time: 0.93(0.93)  Batch time: 4.22(4.22)
2025-07-12 10:18:42,668   INFO  Train:   12/100 ( 12%) [  11/408 (  3%)]  Loss: 7.088 (8.29)  LR: 7.754e-04  Time cost: 00:09/05:25 [28:54/8:16:42]  Acc_iter 4500        Data time: 0.00(0.28)  Forward time: 0.82(0.54)  Batch time: 0.82(0.82)
2025-07-12 10:18:42,965   INFO  
2025-07-12 10:19:09,246   INFO  Train:   12/100 ( 12%) [  61/408 ( 15%)]  Loss: 9.555 (8.26)  LR: 7.854e-04  Time cost: 00:36/03:23 [29:21/5:55:00]  Acc_iter 4550        Data time: 0.00(0.06)  Forward time: 0.70(0.53)  Batch time: 0.70(0.59)
2025-07-12 10:19:36,981   INFO  Train:   12/100 ( 12%) [ 111/408 ( 27%)]  Loss: 7.537 (8.19)  LR: 7.954e-04  Time cost: 01:04/02:50 [29:49/5:45:39]  Acc_iter 4600        Data time: 0.02(0.04)  Forward time: 0.86(0.54)  Batch time: 0.87(0.57)
2025-07-12 10:20:03,328   INFO  Train:   12/100 ( 12%) [ 161/408 ( 39%)]  Loss: 7.429 (8.19)  LR: 8.055e-04  Time cost: 01:30/02:18 [30:15/5:36:38]  Acc_iter 4650        Data time: 0.00(0.03)  Forward time: 0.40(0.53)  Batch time: 0.41(0.56)
2025-07-12 10:20:03,613   INFO  
2025-07-12 10:20:29,576   INFO  Train:   12/100 ( 12%) [ 211/408 ( 52%)]  Loss: 11.88 (8.27)  LR: 8.157e-04  Time cost: 01:56/01:48 [30:41/5:31:22]  Acc_iter 4700        Data time: 0.00(0.02)  Forward time: 0.27(0.53)  Batch time: 0.27(0.55)
2025-07-12 10:20:56,490   INFO  Train:   12/100 ( 12%) [ 261/408 ( 64%)]  Loss: 7.640 (8.26)  LR: 8.259e-04  Time cost: 02:23/01:20 [31:08/5:29:29]  Acc_iter 4750        Data time: 0.00(0.02)  Forward time: 0.34(0.53)  Batch time: 0.34(0.55)
2025-07-12 10:21:23,615   INFO  Train:   12/100 ( 12%) [ 311/408 ( 76%)]  Loss: 7.221 (8.20)  LR: 8.362e-04  Time cost: 02:50/00:53 [31:35/5:28:28]  Acc_iter 4800        Data time: 0.00(0.02)  Forward time: 0.66(0.53)  Batch time: 0.66(0.55)
2025-07-12 10:21:23,925   INFO  
2025-07-12 10:21:50,275   INFO  Train:   12/100 ( 12%) [ 361/408 ( 88%)]  Loss: 7.703 (8.19)  LR: 8.466e-04  Time cost: 03:17/00:25 [32:02/5:26:49]  Acc_iter 4850        Data time: 0.00(0.02)  Forward time: 0.39(0.53)  Batch time: 0.39(0.55)
2025-07-12 10:22:12,577   INFO  Train:   12/100 ( 12%) [ 407/408 (100%)]  Loss: 8.456 (8.17)  LR: 8.563e-04  Time cost: 03:39/00:00 [32:24/5:22:19]  Acc_iter 4896        Data time: 0.00(0.01)  Forward time: 0.12(0.52)  Batch time: 0.12(0.54)
2025-07-12 10:22:16,978   INFO  Train:   13/100 ( 13%) [   0/408 (  0%)]  Loss: 8.433 (8.43)  LR: 8.565e-04  Time cost: 00:03/24:47 [32:29/36:22:14]  Acc_iter 4897        Data time: 2.34(2.34)  Forward time: 1.31(1.31)  Batch time: 3.65(3.65)
2025-07-12 10:22:18,718   INFO  Train:   13/100 ( 13%) [   3/408 (  1%)]  Loss: 7.683 (8.08)  LR: 8.571e-04  Time cost: 00:05/09:05 [32:30/13:25:49]  Acc_iter 4900        Data time: 0.02(0.59)  Forward time: 0.68(0.75)  Batch time: 0.70(1.35)
2025-07-12 10:22:45,321   INFO  Train:   13/100 ( 13%) [  53/408 ( 13%)]  Loss: 8.106 (8.13)  LR: 8.677e-04  Time cost: 00:31/03:30 [32:57/5:53:58]  Acc_iter 4950        Data time: 0.01(0.05)  Forward time: 0.48(0.55)  Batch time: 0.48(0.59)
2025-07-12 10:22:45,645   INFO  
2025-07-12 10:23:12,957   INFO  Train:   13/100 ( 13%) [ 103/408 ( 25%)]  Loss: 7.684 (8.22)  LR: 8.783e-04  Time cost: 00:59/02:54 [33:25/5:42:05]  Acc_iter 5000        Data time: 0.00(0.03)  Forward time: 0.57(0.54)  Batch time: 0.57(0.57)
2025-07-12 10:23:39,444   INFO  Train:   13/100 ( 13%) [ 153/408 ( 38%)]  Loss: 10.34 (8.26)  LR: 8.890e-04  Time cost: 01:26/02:22 [33:51/5:33:10]  Acc_iter 5050        Data time: 0.00(0.02)  Forward time: 0.57(0.54)  Batch time: 0.57(0.56)
2025-07-12 10:24:06,368   INFO  Train:   13/100 ( 13%) [ 203/408 ( 50%)]  Loss: 8.104 (8.16)  LR: 8.998e-04  Time cost: 01:53/01:53 [34:18/5:29:42]  Acc_iter 5100        Data time: 0.00(0.02)  Forward time: 0.42(0.54)  Batch time: 0.42(0.55)
2025-07-12 10:24:06,644   INFO  
2025-07-12 10:24:32,878   INFO  Train:   13/100 ( 13%) [ 253/408 ( 62%)]  Loss: 8.545 (8.13)  LR: 9.106e-04  Time cost: 02:19/01:25 [34:45/5:26:26]  Acc_iter 5150        Data time: 0.00(0.02)  Forward time: 0.41(0.53)  Batch time: 0.41(0.55)
2025-07-12 10:25:00,147   INFO  Train:   13/100 ( 13%) [ 303/408 ( 74%)]  Loss: 6.914 (8.09)  LR: 9.215e-04  Time cost: 02:46/00:57 [35:12/5:25:35]  Acc_iter 5200        Data time: 0.00(0.01)  Forward time: 0.33(0.53)  Batch time: 0.33(0.55)
2025-07-12 10:25:26,373   INFO  Train:   13/100 ( 13%) [ 353/408 ( 87%)]  Loss: 7.730 (8.01)  LR: 9.325e-04  Time cost: 03:13/00:29 [35:38/5:23:06]  Acc_iter 5250        Data time: 0.00(0.01)  Forward time: 0.69(0.53)  Batch time: 0.69(0.55)
2025-07-12 10:25:26,634   INFO  
2025-07-12 10:25:51,922   INFO  Train:   13/100 ( 13%) [ 403/408 ( 99%)]  Loss: 7.965 (8.02)  LR: 9.435e-04  Time cost: 03:38/00:02 [36:04/5:20:08]  Acc_iter 5300        Data time: 0.00(0.01)  Forward time: 0.27(0.53)  Batch time: 0.27(0.54)
2025-07-12 10:25:52,839   INFO  Train:   13/100 ( 13%) [ 407/408 (100%)]  Loss: 6.810 (8.02)  LR: 9.444e-04  Time cost: 03:39/00:00 [36:05/5:18:17]  Acc_iter 5304        Data time: 0.00(0.01)  Forward time: 0.14(0.53)  Batch time: 0.14(0.54)
2025-07-12 10:25:58,010   INFO  Train:   14/100 ( 14%) [   0/408 (  0%)]  Loss: 5.733 (5.73)  LR: 9.446e-04  Time cost: 00:04/28:59 [36:10/42:02:09]  Acc_iter 5305        Data time: 3.31(3.31)  Forward time: 0.95(0.95)  Batch time: 4.26(4.26)
2025-07-12 10:26:22,266   INFO  Train:   14/100 ( 14%) [  45/408 ( 11%)]  Loss: 7.195 (8.07)  LR: 9.546e-04  Time cost: 00:28/03:45 [36:34/6:06:18]  Acc_iter 5350        Data time: 0.05(0.08)  Forward time: 0.91(0.54)  Batch time: 0.96(0.62)
2025-07-12 10:26:49,518   INFO  Train:   14/100 ( 14%) [  95/408 ( 23%)]  Loss: 9.072 (7.93)  LR: 9.658e-04  Time cost: 00:55/03:01 [37:01/5:42:46]  Acc_iter 5400        Data time: 0.00(0.04)  Forward time: 0.47(0.54)  Batch time: 0.48(0.58)
2025-07-12 10:26:49,758   INFO  
2025-07-12 10:27:16,551   INFO  Train:   14/100 ( 14%) [ 145/408 ( 36%)]  Loss: 7.010 (7.90)  LR: 9.770e-04  Time cost: 01:22/02:29 [37:28/5:34:09]  Acc_iter 5450        Data time: 0.00(0.03)  Forward time: 0.27(0.54)  Batch time: 0.27(0.57)
2025-07-12 10:27:42,485   INFO  Train:   14/100 ( 14%) [ 195/408 ( 48%)]  Loss: 7.513 (7.86)  LR: 9.883e-04  Time cost: 01:48/01:58 [37:54/5:26:24]  Acc_iter 5500        Data time: 0.00(0.02)  Forward time: 0.70(0.53)  Batch time: 0.71(0.55)
2025-07-12 10:28:08,668   INFO  Train:   14/100 ( 14%) [ 245/408 ( 60%)]  Loss: 9.537 (7.84)  LR: 9.997e-04  Time cost: 02:14/01:29 [38:20/5:22:13]  Acc_iter 5550        Data time: 0.00(0.02)  Forward time: 0.30(0.53)  Batch time: 0.30(0.55)
2025-07-12 10:28:08,898   INFO  
2025-07-12 10:28:34,416   INFO  Train:   14/100 ( 14%) [ 295/408 ( 72%)]  Loss: 6.212 (7.80)  LR: 1.011e-03  Time cost: 02:40/01:01 [38:46/5:18:27]  Acc_iter 5600        Data time: 0.02(0.02)  Forward time: 0.41(0.53)  Batch time: 0.42(0.54)
2025-07-12 10:29:00,214   INFO  Train:   14/100 ( 14%) [ 345/408 ( 85%)]  Loss: 7.815 (7.83)  LR: 1.023e-03  Time cost: 03:06/00:33 [39:12/5:15:43]  Acc_iter 5650        Data time: 0.01(0.02)  Forward time: 0.65(0.52)  Batch time: 0.65(0.54)
2025-07-12 10:29:25,929   INFO  Train:   14/100 ( 14%) [ 395/408 ( 97%)]  Loss: 7.380 (7.81)  LR: 1.034e-03  Time cost: 03:32/00:06 [39:38/5:13:27]  Acc_iter 5700        Data time: 0.00(0.01)  Forward time: 0.71(0.52)  Batch time: 0.71(0.54)
2025-07-12 10:29:26,250   INFO  
2025-07-12 10:29:30,690   INFO  Train:   14/100 ( 14%) [ 407/408 (100%)]  Loss: 9.242 (7.81)  LR: 1.037e-03  Time cost: 03:36/00:00 [39:42/5:10:57]  Acc_iter 5712        Data time: 0.00(0.01)  Forward time: 0.29(0.52)  Batch time: 0.29(0.53)
2025-07-12 10:29:35,675   INFO  Train:   15/100 ( 15%) [   0/408 (  0%)]  Loss: 6.813 (6.81)  LR: 1.037e-03  Time cost: 00:04/28:54 [39:47/41:26:12]  Acc_iter 5713        Data time: 3.37(3.37)  Forward time: 0.88(0.88)  Batch time: 4.25(4.25)
2025-07-12 10:30:00,838   INFO  Train:   15/100 ( 15%) [  37/408 (  9%)]  Loss: 6.095 (7.20)  LR: 1.046e-03  Time cost: 00:29/04:47 [40:13/7:32:12]  Acc_iter 5750        Data time: 0.00(0.10)  Forward time: 0.58(0.67)  Batch time: 0.58(0.77)
2025-07-12 10:30:32,547   INFO  Train:   15/100 ( 15%) [  87/408 ( 21%)]  Loss: 7.161 (7.28)  LR: 1.057e-03  Time cost: 01:01/03:42 [40:44/6:45:11]  Acc_iter 5800        Data time: 0.00(0.05)  Forward time: 0.39(0.65)  Batch time: 0.39(0.69)
2025-07-12 10:30:58,862   INFO  Train:   15/100 ( 15%) [ 137/408 ( 34%)]  Loss: 8.046 (7.30)  LR: 1.069e-03  Time cost: 01:27/02:51 [41:11/6:09:05]  Acc_iter 5850        Data time: 0.00(0.03)  Forward time: 0.61(0.60)  Batch time: 0.62(0.63)
2025-07-12 10:30:59,085   INFO  
2025-07-12 10:31:24,955   INFO  Train:   15/100 ( 15%) [ 187/408 ( 46%)]  Loss: 7.437 (7.37)  LR: 1.081e-03  Time cost: 01:53/02:13 [41:37/5:51:16]  Acc_iter 5900        Data time: 0.00(0.03)  Forward time: 0.43(0.58)  Batch time: 0.43(0.60)
2025-07-12 10:31:50,649   INFO  Train:   15/100 ( 15%) [ 237/408 ( 58%)]  Loss: 7.459 (7.42)  LR: 1.093e-03  Time cost: 02:19/01:40 [42:02/5:39:47]  Acc_iter 5950        Data time: 0.00(0.02)  Forward time: 0.50(0.56)  Batch time: 0.50(0.58)
2025-07-12 10:32:16,549   INFO  Train:   15/100 ( 15%) [ 287/408 ( 70%)]  Loss: 8.066 (7.47)  LR: 1.104e-03  Time cost: 02:45/01:09 [42:28/5:32:33]  Acc_iter 6000        Data time: 0.00(0.02)  Forward time: 0.67(0.55)  Batch time: 0.67(0.57)
2025-07-12 10:32:16,810   INFO  
2025-07-12 10:32:42,933   INFO  Train:   15/100 ( 15%) [ 337/408 ( 83%)]  Loss: 10.37 (7.51)  LR: 1.116e-03  Time cost: 03:11/00:40 [42:55/5:28:09]  Acc_iter 6050        Data time: 0.00(0.02)  Forward time: 0.42(0.55)  Batch time: 0.43(0.57)
2025-07-12 10:33:09,681   INFO  Train:   15/100 ( 15%) [ 387/408 ( 95%)]  Loss: 7.495 (7.51)  LR: 1.128e-03  Time cost: 03:38/00:11 [43:21/5:25:19]  Acc_iter 6100        Data time: 0.00(0.02)  Forward time: 0.63(0.55)  Batch time: 0.63(0.56)
2025-07-12 10:33:18,012   INFO  Train:   15/100 ( 15%) [ 407/408 (100%)]  Loss: 6.037 (7.53)  LR: 1.133e-03  Time cost: 03:46/00:00 [43:30/5:21:00]  Acc_iter 6120        Data time: 0.00(0.02)  Forward time: 0.16(0.54)  Batch time: 0.16(0.56)
2025-07-12 10:33:22,499   INFO  Train:   16/100 ( 16%) [   0/408 (  0%)]  Loss: 7.402 (7.40)  LR: 1.133e-03  Time cost: 00:03/24:27 [43:34/34:39:32]  Acc_iter 6121        Data time: 2.35(2.35)  Forward time: 1.25(1.25)  Batch time: 3.60(3.60)
2025-07-12 10:33:37,551   INFO  Train:   16/100 ( 16%) [  29/408 (  7%)]  Loss: 7.225 (7.43)  LR: 1.140e-03  Time cost: 00:18/03:55 [43:49/5:59:00]  Acc_iter 6150        Data time: 0.00(0.08)  Forward time: 0.37(0.54)  Batch time: 0.38(0.62)
2025-07-12 10:33:37,850   INFO  
2025-07-12 10:34:03,811   INFO  Train:   16/100 ( 16%) [  79/408 ( 19%)]  Loss: 5.236 (7.24)  LR: 1.152e-03  Time cost: 00:44/03:04 [44:16/5:23:44]  Acc_iter 6200        Data time: 0.00(0.04)  Forward time: 0.44(0.52)  Batch time: 0.44(0.56)
2025-07-12 10:34:29,649   INFO  Train:   16/100 ( 16%) [ 129/408 ( 32%)]  Loss: 6.013 (7.28)  LR: 1.165e-03  Time cost: 01:10/02:31 [44:41/5:13:22]  Acc_iter 6250        Data time: 0.00(0.02)  Forward time: 0.72(0.52)  Batch time: 0.72(0.54)
2025-07-12 10:34:54,778   INFO  Train:   16/100 ( 16%) [ 179/408 ( 44%)]  Loss: 7.225 (7.25)  LR: 1.177e-03  Time cost: 01:35/02:01 [45:07/5:06:16]  Acc_iter 6300        Data time: 0.00(0.02)  Forward time: 0.82(0.51)  Batch time: 0.82(0.53)
2025-07-12 10:34:55,001   INFO  
2025-07-12 10:35:20,142   INFO  Train:   16/100 ( 16%) [ 229/408 ( 56%)]  Loss: 6.128 (7.28)  LR: 1.189e-03  Time cost: 02:01/01:34 [45:32/5:02:40]  Acc_iter 6350        Data time: 0.00(0.02)  Forward time: 0.27(0.51)  Batch time: 0.27(0.53)
2025-07-12 10:35:45,742   INFO  Train:   16/100 ( 16%) [ 279/408 ( 68%)]  Loss: 7.445 (7.25)  LR: 1.201e-03  Time cost: 02:26/01:07 [45:57/5:00:40]  Acc_iter 6400        Data time: 0.00(0.01)  Forward time: 0.39(0.51)  Batch time: 0.39(0.52)
2025-07-12 10:36:11,445   INFO  Train:   16/100 ( 16%) [ 329/408 ( 81%)]  Loss: 8.536 (7.28)  LR: 1.213e-03  Time cost: 02:52/00:41 [46:23/4:59:20]  Acc_iter 6450        Data time: 0.00(0.01)  Forward time: 0.42(0.51)  Batch time: 0.43(0.52)
2025-07-12 10:36:11,665   INFO  
2025-07-12 10:36:36,869   INFO  Train:   16/100 ( 16%) [ 379/408 ( 93%)]  Loss: 5.729 (7.28)  LR: 1.226e-03  Time cost: 03:17/00:15 [46:49/4:57:49]  Acc_iter 6500        Data time: 0.01(0.01)  Forward time: 0.58(0.51)  Batch time: 0.59(0.52)
2025-07-12 10:36:49,328   INFO  Train:   16/100 ( 16%) [ 407/408 (100%)]  Loss: 7.601 (7.31)  LR: 1.233e-03  Time cost: 03:30/00:00 [47:01/4:54:36]  Acc_iter 6528        Data time: 0.00(0.01)  Forward time: 0.16(0.50)  Batch time: 0.16(0.52)
2025-07-12 10:36:53,678   INFO  Train:   17/100 ( 17%) [   0/408 (  0%)]  Loss: 7.492 (7.49)  LR: 1.233e-03  Time cost: 00:03/24:29 [47:05/34:17:07]  Acc_iter 6529        Data time: 2.96(2.96)  Forward time: 0.64(0.64)  Batch time: 3.60(3.60)
2025-07-12 10:37:04,429   INFO  Train:   17/100 ( 17%) [  21/408 (  5%)]  Loss: 5.384 (6.95)  LR: 1.238e-03  Time cost: 00:14/04:12 [47:16/6:12:24]  Acc_iter 6550        Data time: 0.00(0.14)  Forward time: 0.56(0.51)  Batch time: 0.56(0.65)
2025-07-12 10:37:31,131   INFO  Train:   17/100 ( 17%) [  71/408 ( 17%)]  Loss: 6.679 (7.01)  LR: 1.250e-03  Time cost: 00:41/03:12 [47:43/5:25:01]  Acc_iter 6600        Data time: 0.00(0.04)  Forward time: 0.47(0.53)  Batch time: 0.47(0.57)
2025-07-12 10:37:31,353   INFO  
2025-07-12 10:37:57,596   INFO  Train:   17/100 ( 17%) [ 121/408 ( 30%)]  Loss: 6.450 (7.04)  LR: 1.263e-03  Time cost: 01:07/02:38 [48:09/5:15:00]  Acc_iter 6650        Data time: 0.00(0.03)  Forward time: 0.55(0.52)  Batch time: 0.55(0.55)
2025-07-12 10:38:23,044   INFO  Train:   17/100 ( 17%) [ 171/408 ( 42%)]  Loss: 5.018 (7.09)  LR: 1.275e-03  Time cost: 01:32/02:08 [48:35/5:07:11]  Acc_iter 6700        Data time: 0.04(0.02)  Forward time: 0.50(0.52)  Batch time: 0.54(0.54)
2025-07-12 10:38:48,523   INFO  Train:   17/100 ( 17%) [ 221/408 ( 54%)]  Loss: 5.549 (7.10)  LR: 1.288e-03  Time cost: 01:58/01:39 [49:00/5:02:47]  Acc_iter 6750        Data time: 0.00(0.02)  Forward time: 0.47(0.52)  Batch time: 0.48(0.53)
2025-07-12 10:38:48,812   INFO  
2025-07-12 10:39:14,958   INFO  Train:   17/100 ( 17%) [ 271/408 ( 66%)]  Loss: 5.725 (7.05)  LR: 1.300e-03  Time cost: 02:24/01:12 [49:27/5:01:50]  Acc_iter 6800        Data time: 0.00(0.02)  Forward time: 0.88(0.52)  Batch time: 0.88(0.53)
2025-07-12 10:39:40,390   INFO  Train:   17/100 ( 17%) [ 321/408 ( 79%)]  Loss: 6.413 (7.03)  LR: 1.313e-03  Time cost: 02:50/00:46 [49:52/4:59:17]  Acc_iter 6850        Data time: 0.00(0.01)  Forward time: 0.56(0.51)  Batch time: 0.56(0.53)
2025-07-12 10:40:05,952   INFO  Train:   17/100 ( 17%) [ 371/408 ( 91%)]  Loss: 8.047 (7.03)  LR: 1.326e-03  Time cost: 03:15/00:19 [50:18/4:57:30]  Acc_iter 6900        Data time: 0.00(0.01)  Forward time: 0.55(0.51)  Batch time: 0.55(0.53)
2025-07-12 10:40:06,277   INFO  
2025-07-12 10:40:22,472   INFO  Train:   17/100 ( 17%) [ 407/408 (100%)]  Loss: 5.388 (7.05)  LR: 1.335e-03  Time cost: 03:32/00:00 [50:34/4:53:49]  Acc_iter 6936        Data time: 0.00(0.01)  Forward time: 0.15(0.51)  Batch time: 0.15(0.52)
2025-07-12 10:40:27,105   INFO  Train:   18/100 ( 18%) [   0/408 (  0%)]  Loss: 6.807 (6.81)  LR: 1.335e-03  Time cost: 00:03/25:29 [50:39/35:15:54]  Acc_iter 6937        Data time: 2.52(2.52)  Forward time: 1.23(1.23)  Batch time: 3.75(3.75)
2025-07-12 10:40:34,392   INFO  Train:   18/100 ( 18%) [  13/408 (  3%)]  Loss: 7.505 (6.80)  LR: 1.338e-03  Time cost: 00:11/05:11 [50:46/7:24:44]  Acc_iter 6950        Data time: 0.00(0.18)  Forward time: 0.57(0.61)  Batch time: 0.57(0.79)
